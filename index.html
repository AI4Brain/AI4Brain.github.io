<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="./assets/style.css" rel="stylesheet" type="text/css">
    <title>AI4Brain</title>
    
</head>
<body>
    <header>
        <div class="logo">
            <img src="img/ai4brain.png" alt="Logo" class="avatar"> <!-- 用你的Logo图片替换 "logo.png" -->
        </div>
        <div class="banner">
            <div class="title">
            脑科学与智能
            </div>
        </div>
        <div class="container">
            <div class="search-container">
                <input type="text" id="searchInput" placeholder="Search this site...">
                <button id="searchButton"><img src="img/search.png" width="22px"></button>
            </div>
            <ul id="results"></ul>
        </div>
        <script src="./assets/search.js"></script>
    </header>
  
  
    
    
    <div class="projects">
        <h1>Publications</h1>
        <hr>
        <div class="item">
            <div class="image-container">
                <img src="img/SleepKD.png" alt="图片1">
            </div>
            <div class="text-container">
                <h3>IJCAI 2023</h3>
                <h1>Teacher Assistant-Based Knowledge Distillation Extracting Multi-level Features on Single Channel Sleep EEG</h1>
                <h2>Heng Liang, Yucheng Liu, Haichao Wang, Ziyu Jia*</h2>
                <!-- <p>Existing sleep stage classification models are large in size, making it difficult to deploy applications in real-world scenarios. We design a generalized knowledge distillation framework for sleep stage classification models with multi-level distillation module, teacher assistant module, and the rest of the distillation modules. Experiments on two datasets, Sleep-EDF and ISRUC-III, show that this method significantly compress the sleep models with little performance loss.</p> -->
                <div class="'link-container">
                    <a href="https://www.ijcai.org/proceedings/2023/0439.pdf" class="link">Paper</a>
                    <a href="https://github.com/Hychaowang/SleepKD" class="link">Code</a>
                    <a href="https://hychaowang.github.io/SleepKD" class="link">Webpage</a>
                    <a href="https://hychaowang.github.io/SleepKD#video" class="link">Video</a>
                </div>
            </div>
        </div>

        <hr>

        <div class="item">
            <div class="image-container">
                <img src="img/emokd.png" alt="图片1">
            </div>
            <div class="text-container">
                <h3>ACM Multimedia 2023</h3>
                <h1>EmotionKD: A Cross-Modal Knowledge Distillation Framework for Emotion Recognition Based on Physiological Signals</h1>
                <h2>Yucheng Liu, Ziyu Jia*, Haichao Wang</h2>
                <!-- <p>Existing sleep stage classification models are large in size, making it difficult to deploy applications in real-world scenarios. We design a generalized knowledge distillation framework for sleep stage classification models with multi-level distillation module, teacher assistant module, and the rest of the distillation modules. Experiments on two datasets, Sleep-EDF and ISRUC-III, show that this method significantly compress the sleep models with little performance loss.</p> -->
                <div class="'link-container">
                    <a href="" class="link">Paper</a>
                    <a href="https://github.com/YuchengLiu-Alex/EmotionKD" class="link">Code</a>
                    <a href="https://yuchengliu-alex.github.io/EmotionKD/" class="link">Webpage</a>
                    <a href="https://yuchengliu-alex.github.io/EmotionKD/#video" class="link">Video</a>
                </div>
            </div>
        </div>

        <hr>

        <footer>
            &copy; AI4Brain 2023. 
            <a href="https://space.bilibili.com/3493114830392257?spm_id_from=333.337.0.0"><i class="icon"><img src="img/bilibili.png" width=40 height=20></i></a>
        </footer>
</body>
</html>